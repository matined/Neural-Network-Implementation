{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation_step(A_prev: np.ndarray, W: np.ndarray, b: np.ndarray, activation: str = 'tanh') -> tuple:\n",
    "    \"\"\"\n",
    "    Computes a forward propagation step for one layer.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    A_prev : np.array\n",
    "        Activations from the previous layer of shape (size_of_previous_layer, number_of_examples)\n",
    "\n",
    "    W : np.ndarray\n",
    "        Weights of shape (size_of_current_layer, size_of_previous_layer)\n",
    "\n",
    "    b : np.ndarray\n",
    "        Biases og shape (size_of_current_layer, 1)\n",
    "\n",
    "    activation : str\n",
    "        A choice of a activation function (sigmoid, tanh, ReLU, leaky ReLU)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Z : np.ndarray\n",
    "        Pre-activation parameter, input for an activation function of size (size_of_current_layer, 1)\n",
    "\n",
    "    cache : tuple\n",
    "        A tuple containing A_prev, W, B and Z for the backpropagation\n",
    "    \"\"\"\n",
    "    Z = W @ A_prev + b\n",
    "\n",
    "    if activation == 'tanh':\n",
    "        A = np.tanh(Z)\n",
    "\n",
    "    cache = {\n",
    "        'A_prev': A_prev,\n",
    "        'W': W,\n",
    "        'b': b,\n",
    "        'Z': Z\n",
    "    }\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X: np.ndarray, parameters: dict, number_of_layers: int, activation_function: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Performs a forward propagation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray\n",
    "        Input values of shape (number_of_features, number_of_examples)\n",
    "\n",
    "    parameters : dictionary\n",
    "        Weights and biases W1...WL, b1...bL\n",
    "\n",
    "    number_of_layers : int\n",
    "        A number of layers\n",
    "    \n",
    "    activation : str\n",
    "        A choice of a activation function for hidden layers (sigmoid, tanh, ReLU, leaky ReLU)\n",
    "    \"\"\"\n",
    "    caches = []\n",
    "    A = X\n",
    "\n",
    "    for l in range(1, number_of_layers):\n",
    "        A, cache = forward_propagation_step(A, parameters['W' + str(l)], parameters['b' + str(l)], activation_function)\n",
    "        caches.append(cache)\n",
    "\n",
    "    AL, cache = forward_propagation_step(A, parameters['W' + str(number_of_layers)], parameters['b' + str(number_of_layers)], activation_function)\n",
    "\n",
    "    return AL, caches"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aa87bef9c302eb7f9e205196104a829232ac0295b4f8169b8d0ff13e707d6bb3"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('NN_impl': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
