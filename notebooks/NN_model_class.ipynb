{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from activation_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN_model:\n",
    "    def __init__(self, layers_sizes = (10, 1), learning_rate = 0.075, max_iter = 2000, activation = 'relu', verbose = True) -> None:\n",
    "        self.layer_sizes = layers_sizes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        self.activation = activation\n",
    "        self.verbose = verbose\n",
    "\n",
    "        self.data = {}\n",
    "        self.parameters = {}\n",
    "        self.number_of_layers = len(layers_sizes)\n",
    "\n",
    "    def process_data(self, X: np.ndarray, y: np.ndarray) -> dict:\n",
    "        \"\"\"\n",
    "        Reshapes the input arrays for the simplicity of implementation and creates a few data parameters to use later.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray\n",
    "            Training set, numpy array of shape (number_of_examples, number_of_features) \n",
    "\n",
    "        y : np.ndarray\n",
    "            Target values for the test set, numpy array of shape (number_of_examples, 1)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        data : dictionary\n",
    "            A dictionary containing processed data:\n",
    "                X : numpy array of shape (number_of_features, number_of_examples) \n",
    "                y : numpy array of shape (1, number_of_examples) \n",
    "                m : number_of_training_examples\n",
    "                n_x : number_of_features\n",
    "        \"\"\"\n",
    "\n",
    "        X = X.T\n",
    "        m = X.shape[1]\n",
    "        n_x = X.shape[0]\n",
    "\n",
    "        y= y.reshape(1, m)\n",
    "\n",
    "        data = {\n",
    "            'X':    X,\n",
    "            'y':    y,\n",
    "            'm':    m,\n",
    "            'n_x':  n_x\n",
    "        }\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "    def initialize_parameters(self, layer_dims: tuple) -> dict:\n",
    "        \"\"\"\n",
    "        Randomly initializes parameters for the NN.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer_dims : tuple\n",
    "            A tuple containing numbers of units in the consecutive layers\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        parameters : dictionary\n",
    "            A dictionary containing parameters W1...WL, b1...bL for the NN.\n",
    "                Wn : numpy array of weights for n-th layer\n",
    "                bn : numpy array of biases for n-th layer\n",
    "        \"\"\"\n",
    "        L = len(layer_dims)  # total number of layers\n",
    "        parameters = {}\n",
    "\n",
    "        for l in range(1, L):\n",
    "            parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n",
    "            parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "\n",
    "        return parameters\n",
    "\n",
    "\n",
    "    def forward_propagation_step(self, A_prev: np.ndarray, W: np.ndarray, b: np.ndarray, activation: str = 'relu') -> tuple:\n",
    "        \"\"\"\n",
    "        Computes a forward propagation step for one layer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        A_prev : np.array\n",
    "            Activations from the previous layer of shape (size_of_previous_layer, number_of_examples)\n",
    "\n",
    "        W : np.ndarray\n",
    "            Weights of shape (size_of_current_layer, size_of_previous_layer)\n",
    "\n",
    "        b : np.ndarray\n",
    "            Biases og shape (size_of_current_layer, 1)\n",
    "\n",
    "        activation : str\n",
    "            A choice of a activation function (sigmoid, tanh, ReLU, leaky ReLU)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Z : np.ndarray\n",
    "            Pre-activation parameter, input for an activation function of size (size_of_current_layer, 1)\n",
    "\n",
    "        cache : tuple\n",
    "            A tuple containing A_prev, W, B and Z for the backpropagation\n",
    "        \"\"\"\n",
    "        Z = W @ A_prev + b\n",
    "\n",
    "        if activation == 'tanh':\n",
    "            A = np.tanh(Z)\n",
    "\n",
    "        cache = {\n",
    "            'A_prev': A_prev,\n",
    "            'W': W,\n",
    "            'b': b,\n",
    "            'Z': Z\n",
    "        }\n",
    "\n",
    "        return A, cache\n",
    "\n",
    "\n",
    "    def forward_propagation(self, X: np.ndarray, parameters: dict, number_of_layers: int, activation_function: str) -> tuple:\n",
    "        \"\"\"\n",
    "        Performs a forward propagation.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray\n",
    "            Input values of shape (number_of_features, number_of_examples)\n",
    "\n",
    "        parameters : dictionary\n",
    "            Weights and biases W1...WL, b1...bL\n",
    "\n",
    "        number_of_layers : int\n",
    "            A number of layers\n",
    "\n",
    "        activation : str\n",
    "            A choice of a activation function for hidden layers (sigmoid, tanh, ReLU, leaky ReLU)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        AL : np.ndarrat\n",
    "            Predictions\n",
    "        \n",
    "        caches : list\n",
    "            Caches for BP\n",
    "        \"\"\"\n",
    "        caches = []\n",
    "        A = X\n",
    "\n",
    "        for l in range(1, number_of_layers):\n",
    "            A, cache = self.forward_propagation_step(A, parameters['W' + str(l)], parameters['b' + str(l)], activation_function)\n",
    "            caches.append(cache)\n",
    "\n",
    "        AL, cache = self.forward_propagation_step(A, parameters['W' + str(number_of_layers)], parameters['b' + str(number_of_layers)], activation_function)\n",
    "\n",
    "        return AL, caches\n",
    "\n",
    "\n",
    "    def compute_cost(self, AL: np.ndarray, y: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Computes cost.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        AL : np.ndarray\n",
    "            Probabilities corresponding to the label predictions of shape (1, number_of_examples)\n",
    "\n",
    "        y : np.ndarray\n",
    "            Target values of predictions of shape (1 , number_of_examples)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        cost : int\n",
    "            Cross-entropy cost\n",
    "        \"\"\"\n",
    "        m = y.shape[1]\n",
    "        cost = np.sum(y * np.log(AL) + (1+y) * np.log(1 - AL)) / (-m)\n",
    "        cost = np.squeeze(cost)\n",
    "\n",
    "        return cost\n",
    "\n",
    "\n",
    "    def backpropagation_step(self, dA: np.ndarray, cache: dict, activation: str = 'relu') -> tuple:\n",
    "        \"\"\"\n",
    "        Performs a backpropagation step, computes gradients.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : np.ndarray\n",
    "            Gradient computed ealier.\n",
    "\n",
    "        cache : dictionary\n",
    "            Tuple of values A_prev, W, b and Z calculated during the forward pass.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dA_prev : np.ndarray\n",
    "            Gradient of the cost w.r.t the activation\n",
    "\n",
    "        dW : np.ndarray\n",
    "            Gradient of the cost w.r.t W\n",
    "\n",
    "        db : np.ndarray\n",
    "            Gradient of the cost w.r.t b \n",
    "        \"\"\"\n",
    "        A_prev, W, b, Z = cache['A_prev'], cache['W'], cache['b'], cache['Z']\n",
    "        m = A_prev.shape[1]\n",
    "\n",
    "        if activation == 'relu':\n",
    "            dZ = dA * relu_backward(dA, Z)\n",
    "        elif activation == 'sigmoid':\n",
    "            dZ = dA * sigmoid_backward(dA, Z)\n",
    "\n",
    "        dW = dZ @ A_prev.T / m\n",
    "        db = np.sum(dZ, axis=1, keepdims=True) / m\n",
    "        dA_prev = W.T @ dZ\n",
    "\n",
    "        return dA_prev, dW, db\n",
    "\n",
    "\n",
    "    def backpropagation(self, AL: np.ndarray, y: np.ndarray, caches: list, number_of_layers: int) -> dict:\n",
    "        \"\"\"\n",
    "        Perfoms a full backpropagation.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        AL : np.ndarray\n",
    "            Output of the forward propagation\n",
    "\n",
    "        y : nd.ndarray\n",
    "            Target prediction values\n",
    "\n",
    "        caches : dictionary\n",
    "            List of caches from the forward propagation\n",
    "\n",
    "        number_of_layers : int\n",
    "            A number of layers\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        grads : dictionary\n",
    "            A dictionary of gradients from every step of BP\n",
    "        \"\"\"\n",
    "        grads = {}\n",
    "        m = AL.shape[1]\n",
    "        y = y.reshape(AL.shape)\n",
    "\n",
    "        dAL = -(y / AL - (1 - y) / (1 - AL))\n",
    "\n",
    "        current_cache = caches[number_of_layers-1]\n",
    "        dA_prev_temp, dW_temp, db_temp = self.backpropagation_step(dAL, current_cache, 'sigmoid')\n",
    "        grads[\"dA\" + str(number_of_layers-1)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(number_of_layers)] = dW_temp\n",
    "        grads[\"db\" + str(number_of_layers)] = db_temp\n",
    "\n",
    "        for l in reversed(range(number_of_layers-1)):\n",
    "            current_cache = caches[l]\n",
    "            dA_prev_temp, dW_temp, db_temp = self.backpropagation_step(dA_prev_temp, current_cache, 'relu')\n",
    "            grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "            grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "            grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "        return grads\n",
    "\n",
    "\n",
    "    def update_parameters(self, params: dict, grads: dict, learning_rate: float = 0.0075) -> dict:\n",
    "        \"\"\"\n",
    "        Update parameters using gradient descent.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        params : dictionary\n",
    "            Parameters W1...WL, b1...bL\n",
    "\n",
    "        grads : dictionary\n",
    "            Output of the BP, gradients dA1...dAL, dW1...dWL, db1...dbL\n",
    "\n",
    "        learning_rate : float\n",
    "            A learning rate.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        parameters : dictionary\n",
    "            Updated parameters.\n",
    "        \"\"\"\n",
    "        parameters = params.copy()\n",
    "        L = len(parameters) // 2\n",
    "\n",
    "        for l in range(L):\n",
    "            parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads['dW' + str(l+1)]\n",
    "            parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads['db' + str(l+1)]\n",
    "\n",
    "        return parameters\n",
    "\n",
    "\n",
    "    def fit(self, X: np.ndarray, y:np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Fits model to the training data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.array\n",
    "            The input data as a numpy array of shape (number_of_training_examples, number_of_features)\n",
    "\n",
    "        y : np.array\n",
    "            Target values for the training examples of shape (number_of_training_examples, )\n",
    "        \"\"\"\n",
    "        self.data = self.process_data(X, y)\n",
    "        self.parameters = self.initialize_parameters(self.layer_sizes)\n",
    "\n",
    "        for i in range(self.max_iter):\n",
    "            AL, caches = self.forward_propagation(X, self.parameters, self.number_of_layers, self.activation)\n",
    "            cost = self.compute_cost(AL, y)\n",
    "            grads = self.backpropagation(AL, y, caches, self.number_of_layers)\n",
    "            self.parameters = self.update_parameters(self.parameters, grads, self.learning_rate)\n",
    "\n",
    "            if self.verbose and i % 100 == 0:\n",
    "                print(f'Cost after interation {i}: {np.squeeze(cost)}')\n",
    "            \n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Predicts outputs on the set X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.array\n",
    "            Input data of shape (number_of_training_examples, number_of_features)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        predictions : np.ndarray\n",
    "        \"\"\"\n",
    "        data = self.process_data(X, np.zeros(1))\n",
    "        X = data['X']\n",
    "\n",
    "        predictions = self.forward_propagation(X, self.parameters, self.number_of_layers, self.activation)\n",
    "\n",
    "        return predictions"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aa87bef9c302eb7f9e205196104a829232ac0295b4f8169b8d0ff13e707d6bb3"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('NN_impl': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
