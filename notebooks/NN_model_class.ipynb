{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from activation_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN_model:\n",
    "    def __init__(self, layers_sizes = (10, 1), learning_rate = 0.075, max_iter = 2000, activation = 'relu', verbose = True) -> None:\n",
    "        self.layer_sizes = layers_sizes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        self.activation = activation\n",
    "        self.verbose = verbose\n",
    "\n",
    "        self.data = {}\n",
    "        self.parameters = {}\n",
    "        self.number_of_layers = len(layers_sizes)\n",
    "\n",
    "    def process_data(self, X: np.ndarray, y: np.ndarray) -> dict:\n",
    "        \"\"\"\n",
    "        Reshapes the input arrays for the simplicity of implementation and creates a few data parameters to use later.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray\n",
    "            Training set, numpy array of shape (number_of_examples, number_of_features) \n",
    "\n",
    "        y : np.ndarray\n",
    "            Target values for the test set, numpy array of shape (number_of_examples, 1)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        data : dictionary\n",
    "            A dictionary containing processed data:\n",
    "                X : numpy array of shape (number_of_features, number_of_examples) \n",
    "                y : numpy array of shape (1, number_of_examples) \n",
    "                m : number_of_training_examples\n",
    "                n_x : number_of_features\n",
    "        \"\"\"\n",
    "\n",
    "        X = X.T\n",
    "        m = X.shape[1]\n",
    "        n_x = X.shape[0]\n",
    "\n",
    "        y= y.reshape(1, m)\n",
    "\n",
    "        data = {\n",
    "            'X':    X,\n",
    "            'y':    y,\n",
    "            'm':    m,\n",
    "            'n_x':  n_x\n",
    "        }\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "    def initialize_parameters(self, layer_dims: tuple) -> dict:\n",
    "        \"\"\"\n",
    "        Randomly initializes parameters for the NN.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer_dims : tuple\n",
    "            A tuple containing numbers of units in the consecutive layers\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        parameters : dictionary\n",
    "            A dictionary containing parameters W1...WL, b1...bL for the NN.\n",
    "                Wn : numpy array of weights for n-th layer\n",
    "                bn : numpy array of biases for n-th layer\n",
    "        \"\"\"\n",
    "        L = len(layer_dims)  # total number of layers\n",
    "        parameters = {}\n",
    "\n",
    "        for l in range(1, L):\n",
    "            parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n",
    "            parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "\n",
    "        return parameters\n",
    "\n",
    "\n",
    "    def forward_propagation_step(self, A_prev: np.ndarray, W: np.ndarray, b: np.ndarray, activation: str = 'relu') -> tuple:\n",
    "        \"\"\"\n",
    "        Computes a forward propagation step for one layer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        A_prev : np.array\n",
    "            Activations from the previous layer of shape (size_of_previous_layer, number_of_examples)\n",
    "\n",
    "        W : np.ndarray\n",
    "            Weights of shape (size_of_current_layer, size_of_previous_layer)\n",
    "\n",
    "        b : np.ndarray\n",
    "            Biases og shape (size_of_current_layer, 1)\n",
    "\n",
    "        activation : str\n",
    "            A choice of a activation function (sigmoid, tanh, ReLU, leaky ReLU)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Z : np.ndarray\n",
    "            Pre-activation parameter, input for an activation function of size (size_of_current_layer, 1)\n",
    "\n",
    "        cache : tuple\n",
    "            A tuple containing A_prev, W, B and Z for the backpropagation\n",
    "        \"\"\"\n",
    "        Z = W @ A_prev + b\n",
    "\n",
    "        if activation == 'tanh':\n",
    "            A = np.tanh(Z)\n",
    "\n",
    "        cache = {\n",
    "            'A_prev': A_prev,\n",
    "            'W': W,\n",
    "            'b': b,\n",
    "            'Z': Z\n",
    "        }\n",
    "\n",
    "        return A, cache\n",
    "\n",
    "\n",
    "    def forward_propagation(self, X: np.ndarray, parameters: dict, number_of_layers: int, activation_function: str) -> tuple:\n",
    "        \"\"\"\n",
    "        Performs a forward propagation.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray\n",
    "            Input values of shape (number_of_features, number_of_examples)\n",
    "\n",
    "        parameters : dictionary\n",
    "            Weights and biases W1...WL, b1...bL\n",
    "\n",
    "        number_of_layers : int\n",
    "            A number of layers\n",
    "\n",
    "        activation : str\n",
    "            A choice of a activation function for hidden layers (sigmoid, tanh, ReLU, leaky ReLU)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        AL : np.ndarrat\n",
    "            Predictions\n",
    "        \n",
    "        caches : list\n",
    "            Caches for BP\n",
    "        \"\"\"\n",
    "        caches = []\n",
    "        A = X\n",
    "\n",
    "        for l in range(1, number_of_layers):\n",
    "            A, cache = self.forward_propagation_step(A, parameters['W' + str(l)], parameters['b' + str(l)], activation_function)\n",
    "            caches.append(cache)\n",
    "\n",
    "        AL, cache = self.forward_propagation_step(A, parameters['W' + str(number_of_layers)], parameters['b' + str(number_of_layers)], activation_function)\n",
    "\n",
    "        return AL, caches\n",
    "\n",
    "\n",
    "    def compute_cost(self, AL: np.ndarray, y: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Computes cost.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        AL : np.ndarray\n",
    "            Probabilities corresponding to the label predictions of shape (1, number_of_examples)\n",
    "\n",
    "        y : np.ndarray\n",
    "            Target values of predictions of shape (1 , number_of_examples)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        cost : int\n",
    "            Cross-entropy cost\n",
    "        \"\"\"\n",
    "        m = y.shape[1]\n",
    "        cost = np.sum(y * np.log(AL) + (1+y) * np.log(1 - AL)) / (-m)\n",
    "        cost = np.squeeze(cost)\n",
    "\n",
    "        return cost\n",
    "\n",
    "\n",
    "    def backpropagation_step(self, dA: np.ndarray, cache: dict, activation: str = 'relu') -> tuple:\n",
    "        \"\"\"\n",
    "        Performs a backpropagation step, computes gradients.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : np.ndarray\n",
    "            Gradient computed ealier.\n",
    "\n",
    "        cache : dictionary\n",
    "            Tuple of values A_prev, W, b and Z calculated during the forward pass.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dA_prev : np.ndarray\n",
    "            Gradient of the cost w.r.t the activation\n",
    "\n",
    "        dW : np.ndarray\n",
    "            Gradient of the cost w.r.t W\n",
    "\n",
    "        db : np.ndarray\n",
    "            Gradient of the cost w.r.t b \n",
    "        \"\"\"\n",
    "        A_prev, W, b, Z = cache['A_prev'], cache['W'], cache['b'], cache['Z']\n",
    "        m = A_prev.shape[1]\n",
    "\n",
    "        if activation == 'relu':\n",
    "            dZ = dA * relu_backward(dA, Z)\n",
    "        elif activation == 'sigmoid':\n",
    "            dZ = dA * sigmoid_backward(dA, Z)\n",
    "\n",
    "        dW = dZ @ A_prev.T / m\n",
    "        db = np.sum(dZ, axis=1, keepdims=True) / m\n",
    "        dA_prev = W.T @ dZ\n",
    "\n",
    "        return dA_prev, dW, db\n",
    "\n",
    "\n",
    "    def backpropagation(self, AL: np.ndarray, y: np.ndarray, caches: list, number_of_layers: int) -> dict:\n",
    "        \"\"\"\n",
    "        Perfoms a full backpropagation.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        AL : np.ndarray\n",
    "            Output of the forward propagation\n",
    "\n",
    "        y : nd.ndarray\n",
    "            Target prediction values\n",
    "\n",
    "        caches : dictionary\n",
    "            List of caches from the forward propagation\n",
    "\n",
    "        number_of_layers : int\n",
    "            A number of layers\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        grads : dictionary\n",
    "            A dictionary of gradients from every step of BP\n",
    "        \"\"\"\n",
    "        grads = {}\n",
    "        m = AL.shape[1]\n",
    "        y = y.reshape(AL.shape)\n",
    "\n",
    "        dAL = -(y / AL - (1 - y) / (1 - AL))\n",
    "\n",
    "        current_cache = caches[number_of_layers-1]\n",
    "        dA_prev_temp, dW_temp, db_temp = self.backpropagation_step(dAL, current_cache, 'sigmoid')\n",
    "        grads[\"dA\" + str(number_of_layers-1)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(number_of_layers)] = dW_temp\n",
    "        grads[\"db\" + str(number_of_layers)] = db_temp\n",
    "\n",
    "        for l in reversed(range(number_of_layers-1)):\n",
    "            current_cache = caches[l]\n",
    "            dA_prev_temp, dW_temp, db_temp = self.backpropagation_step(dA_prev_temp, current_cache, 'relu')\n",
    "            grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "            grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "            grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "        return grads\n",
    "\n",
    "\n",
    "    def update_parameters(self, params: dict, grads: dict, learning_rate: float = 0.0075) -> dict:\n",
    "        \"\"\"\n",
    "        Update parameters using gradient descent.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        params : dictionary\n",
    "            Parameters W1...WL, b1...bL\n",
    "\n",
    "        grads : dictionary\n",
    "            Output of the BP, gradients dA1...dAL, dW1...dWL, db1...dbL\n",
    "\n",
    "        learning_rate : float\n",
    "            A learning rate.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        parameters : dictionary\n",
    "            Updated parameters.\n",
    "        \"\"\"\n",
    "        parameters = params.copy()\n",
    "        L = len(parameters) // 2\n",
    "\n",
    "        for l in range(L):\n",
    "            parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads['dW' + str(l+1)]\n",
    "            parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads['db' + str(l+1)]\n",
    "\n",
    "        return parameters\n",
    "\n",
    "\n",
    "    def fit(self, X: np.ndarray, y:np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Fits model to the training data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.array\n",
    "            The input data as a numpy array of shape (number_of_training_examples, number_of_features)\n",
    "\n",
    "        y : np.array\n",
    "            Target values for the training examples of shape (number_of_training_examples, )\n",
    "        \"\"\"\n",
    "        self.data = self.process_data(X, y)\n",
    "        self.parameters = self.initialize_parameters(self.layer_sizes)\n",
    "\n",
    "        for i in range(self.max_iter):\n",
    "            AL, caches = self.forward_propagation(self.data['X'], self.parameters, self.number_of_layers, self.activation)\n",
    "            cost = self.compute_cost(AL, self.data['y'])\n",
    "            grads = self.backpropagation(AL, self.data['y'], caches, self.number_of_layers)\n",
    "            self.parameters = self.update_parameters(self.parameters, grads, self.learning_rate)\n",
    "\n",
    "            if self.verbose and i % 100 == 0:\n",
    "                print(f'Cost after interation {i}: {np.squeeze(cost)}')\n",
    "            \n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Predicts outputs on the set X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.array\n",
    "            Input data of shape (number_of_training_examples, number_of_features)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        predictions : np.ndarray\n",
    "        \"\"\"\n",
    "        data = self.process_data(X, np.zeros(1))\n",
    "        X = data['X']\n",
    "\n",
    "        predictions = self.forward_propagation(X, self.parameters, self.number_of_layers, self.activation)\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.arange(9).reshape(3, 3)\n",
    "y = np.array([[1, 0, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NN_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 3 is different from 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_28411/2489992722.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_28411/92634318.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m             \u001b[0mAL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber_of_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m             \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_cost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackpropagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber_of_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_28411/92634318.py\u001b[0m in \u001b[0;36mforward_propagation\u001b[0;34m(self, X, parameters, number_of_layers, activation_function)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m             \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_propagation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m             \u001b[0mcaches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_28411/92634318.py\u001b[0m in \u001b[0;36mforward_propagation_step\u001b[0;34m(self, A_prev, W, b, activation)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0mA\u001b[0m \u001b[0mtuple\u001b[0m \u001b[0mcontaining\u001b[0m \u001b[0mA_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mZ\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mbackpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \"\"\"\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mW\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mA_prev\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tanh'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 3 is different from 10)"
     ]
    }
   ],
   "source": [
    "model.fit(X, y)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aa87bef9c302eb7f9e205196104a829232ac0295b4f8169b8d0ff13e707d6bb3"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('NN_impl': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
